[
  {
    "objectID": "posts/quira2020/index.html",
    "href": "posts/quira2020/index.html",
    "title": "Inflation Density Forecast for the Dominican Republic",
    "section": "",
    "text": "Ever since countries have adopted the Inflation Targeting Scheme for monetary policy, inflation and inflation forecast has played a central role for central banks. In recent years, the literature is pointing out that not only inflation trajectory is important, but also the uncertainty that surrounds inflation projections.\nHall and Mitchell (2007) and other authors (Rossi (2014)) propose the use of fancharts to show the central path of forecast, but also the uncertainty of these projections (on shaded areas).\nIn this document, my dear friend Nerys and I use a normal density function, with asymmetry and changing variance in time to obtain the density of the inflation forecast and the uncertainty related to the historic forecast error on different horizons."
  },
  {
    "objectID": "posts/la_dash/index.html",
    "href": "posts/la_dash/index.html",
    "title": "Economic Dashboard for Latinamerica",
    "section": "",
    "text": "Recently I’ve been working on Shiny Apps developing dashboards and other kinds of visualizations. I was able to develop this dashboard using data from the CMCA database. This dashboard summarizes the information of some countries.\n\n\n\nLatin America Dashboard\n\n\nI used this project to practice using Shiny modules to create graphs and realized that I need to work on my CSS abilities. Even though I know how to use CSS, it is very challenging for me to define how i would like things to look like, so it’s almost impossible to come up with some “good-looking” designs.\nSomething else that I learned with this project was that I could use functions to reduce writing (and the probability of making a typing mistake). For example, in the UI section of the app, I had to write the same code for each country. Instead of this one can write a function containing how one country should look like and then pass it to a list of countries."
  },
  {
    "objectID": "posts/mipa-2021/index.html",
    "href": "posts/mipa-2021/index.html",
    "title": "Methodology for forecasting short-term inflation using a bottom-up approach",
    "section": "",
    "text": "Inflation is a generalized increase in prices, measured from the relative variations of the price indexes of the products in a representative basket. This indicator is used to update contracts and make projections by agents in the economy.\nThis document (Quinonez and Ramirez 2021) presents one of the main short-term inflation forecasting methodologies used by the Central Bank of the Dominican Republic (BCRD), showing in detail the forecasting process.\nThe development of these price projections for each item of the basket of goods and services, in general, consists of the following steps:\nAfter this, the projection of each item is added using the weights of each one, thus obtaining a short-term inflation projection."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Un poco de todo",
    "section": "",
    "text": "economics\n\n\nGARCH\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2022\n\n\nJuan Quinonez\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit score\n\n\nbanking\n\n\nmoney\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCredit Score\n\n\nEconomics\n\n\nInformation Asymmetry\n\n\nBanking\n\n\n\n\nThe aim is to explain basic economic concepts in order to understand more fluently why a credit score is important.\n\n\n\n\n\n\nSep 5, 2022\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\neconomics\n\n\ndashboard\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2022\n\n\nJuan Quinonez\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninflation\n\n\nforecast\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\nJuan Quinonez\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninflation\n\n\ndensity\n\n\nforecast\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\nJuan Quinonez\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Professional Experience\nDivision Chief  August 2022 - Current\nCentral Bank of the Dominican Republic  Monetary Programming and Economic Studies Department\nTechnical Assistant  May 2019 - August 2022\nCentral Bank of the Dominican Republic  Monetary Programming and Economic Studies Department\nCredit Risk Manager  February 2018 - May 2019\nBanco Múltiple Promerica  Risk Department\nEconomic Studies Manager  November 2016 - February 2018\nBanco BHD León  Risk Department\nModelling Manager  December 2015 - November 2016\nBanco BHD León  Risk Department"
  },
  {
    "objectID": "series-cs.html",
    "href": "series-cs.html",
    "title": "Credit Score Series",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit score\n\n\nbanking\n\n\nmoney\n\n\n \n\n\n\n\nSep 7, 2022\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCredit Score\n\n\nEconomics\n\n\nInformation Asymmetry\n\n\nBanking\n\n\n \n\n\n\n\nSep 5, 2022\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/series-cs/importance/index.html",
    "href": "posts/series-cs/importance/index.html",
    "title": "The Importance of a Credit Score",
    "section": "",
    "text": "However, companies do not always have complete information about customers, so they cannot perfectly discriminate between good customers and bad customers. This puts companies on the downside of adverse selection in this type of business.\nCredit scores opportunistically seek to correct the adverse selection problems of these entities by summarizing a set of customer characteristics into a score. The central idea of these scales is to score customers according to a set of characteristics and to identify those attributes inherent in good and bad customers. Bessis (2015) defines credit ratings as assessments of the credit standing of borrowers and are excellent tools that best discriminate between defaulters and non-defaulters. The use of credit scores offers an objective way to assess risk, and it is also a consistent approach (Siddiqi 2006) that can be applied to a great number of potential customers in a relative simple procedure (thanks to the technological advances in recent years).\nOn the other hand, clients seek to improve their scores by making various modifications to their financial behavior (paying debts in arrears, or improving their use of revolving credit, among others), so that companies will offer their services to them.\nOnce selected, clients can negotiate additional services or lower prices, given their score level, i.e. clients of a commercial bank with a better credit score can receive preferential lending rates.\n\n\nTable 1: Credit Score\n\n\n\n\n\n\n\n\n\nScore\nNumber of Good Clients\nCumulative Number of Good Clients\nNumber of Bad Clients\nCumulative Number of Bad Clients\n\n\n\n\n175 - 200\n115\n115\n10\n10\n\n\n150 - 174\n105\n220\n20\n30\n\n\n125 - 149\n95\n315\n30\n60\n\n\n100 - 124\n85\n400\n40\n100\n\n\n75 - 99\n75\n475\n50\n150\n\n\n50 - 74\n65\n540\n60\n210\n\n\n0 - 49\n55\n595\n70\n280\n\n\n\n\nAuthor’s elaboration\nTable 1 shows an example of a credit score and the cumulative number of good and bad clients. Using this information businesses may choose to serve only clients with credit scores above 125. This would mean that they will serve 375 clients, 315 will be good clients and 60 will be bad clients, which implies that, given the cut-off score, 16% of the clients may be bad clients. Low cut-off score will increase the potential clients, but will also increase the chance of getting bad clients.\nThis table also shows that for every score, there are good and also bad clients, which leads to interpret credit scores as a relative risk metric, indicating the likelihood of a consumer that will not be able to honor its obligations in the short-term. Hence, this score is related to the probability of default, usually, the greater the score, the lower the probability of default.\nAs a business standard, businesses set a score range in which expert analysis is needed to make a decision. Credit ratings, in sum, are a mix of qualitative and quantitative criteria used to assess the potential clients, but should also expert analysis at the end of the decision process.\nCredit scores help asses client creditworthiness, and also allows clients to access better pricing for services. As mentioned above, this metrics summarizes the characteristics of a potential customer, but it does not take into account the macroeconomic context. This means that it is possible that one can find a great customer, with excellent credit score, but cannot honor his debts due to an economic crisis that caused him to loose his job. This limitations should be taken into consideration for further research.\nHow are credit scores calibrated?\nWhich characteristics influence in this scale?\nThese, and more questions are soon to be answered in latter posts.\n\n\n\n\nReferences\n\nBessis, Joël. 2015. “Risk Management in Banking.”\n\n\nSiddiqi, Nadim. 2006. “Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring.”\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{díaztavera2022,\n  author = {César Díaz Tavera and Juan Quiñonez Wu},\n  editor = {},\n  title = {The {Importance} of a {Credit} {Score}},\n  date = {2022-09-07},\n  url = {https://juansqw.github.io/jquinonez/posts/series-cs/importance/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCésar Díaz Tavera, and Juan Quiñonez Wu. 2022. “The Importance of\na Credit Score.” September 7, 2022. https://juansqw.github.io/jquinonez/posts/series-cs/importance/."
  },
  {
    "objectID": "posts/series-cs/intro-to-the-study-serie/09-05-2022_intro_study_serie.html",
    "href": "posts/series-cs/intro-to-the-study-serie/09-05-2022_intro_study_serie.html",
    "title": "Introduction to the study serie: key concepts to understand credit scoring",
    "section": "",
    "text": "The sustainable and healthy development of a country’s financial system and banking sector is of utmost importance for economic growth. As banks are the main source of credit, they provide economic agents with funds to consume and invest, adding dynamism to the economic activity. However, the banking sector is full of risks. That is why risk management is an area of extreme importance and has remained a topic of major interest since the financial crisis of 2007-2008. Today, risk management is one of the central functions of financial institutions.\nBecause of its importance, Cesar Díaz and I have decided to carry out a brief series of studies where we consider a type of risk and, specifically, one of the ways in which it can be mitigated. We refer to credit risk and credit scores, respectively. This study series on credit score in banking will cover the following:\n\nIntroduction to the study series: key concepts to understand credit scoring\nThe Importance of a Credit Score\nMethodologies for Credit Score development\n\nLogit model\n\nOut of sample evaluation\nKolmogorov - Smirnov test\n\nMachine learning models\nLogit benchmark vs ML models\n\n\nThe goal of the study series is to better understand what a credit score is, why it is important, and how it is constructed. Explicitly, after briefly laying out a theoretical background on its importance, we aim to delved into the statistical techniques that are appropriate to construct a good credit score system. This article is the first in the series of studies and reviews the key economic concepts involved in the underlying problem that score credits try to mitigate."
  },
  {
    "objectID": "posts/series-cs/intro-to-the-study-serie/09-05-2022_intro_study_serie.html#definition-of-key-concepts",
    "href": "posts/series-cs/intro-to-the-study-serie/09-05-2022_intro_study_serie.html#definition-of-key-concepts",
    "title": "Introduction to the study serie: key concepts to understand credit scoring",
    "section": "Definition of key concepts",
    "text": "Definition of key concepts\nInformation asymmetry cause the healthy functioning of markets to disappear.\nInformation is the basis of decision making. Whenever we want to do a transaction of any kind1 we try to collect as much information as possible in order to gain bargain power, or simply to be sure that the transaction is beneficial. Information asymmetry refers to a situation of imbalance in which, in a transaction of any kind, one of the interested parties has more or better information than the other, which creates inefficiency and even market failures in some cases. Popular examples of information asymmetry are moral hazard and adverse selection.\nOn the other hand, adverse selection is a pre-contractual problem that leads to bad decision2. Since Akerlof (1978) seminal article ‘the market for lemons’3, where he explains the concept and its implications in market dynamics, adverse selection has come to be seen as a fundamental cause of market failure. Housing, insurance, credit markets and others suffer constantly from this problem. Put simply, adverse selection is the problem of correctly distinguish between a good and a bad product4 when there is information asymmetry in an economic transaction. This problem arises because sometimes it is very costly/difficult to be fully informed about the quality or features of a product. In markets where this problem is consistent, the overall quality of the products at sell in the market significantly declines5. In the banking industry, adverse selection is common as an issue concerning the provision of credit, since the intermediary can’t know for sure who has a high probability of default.\nMoral hazard is a post-contractual information asymmetry problem that occurs when one of the parties to a contract takes actions that represent additional risk (and thus are detrimental) to the other party, within the framework of what had been agreed. For example, if a borrower takes out a loan with the idea of starting a retail business with which he would pay the installment, but uses the money to travel, his probability of default increases, hence increasing in the lender’s credit risk.\nThus, these problems of information asymmetry permeate the financial markets with risk. Risk can be defined as the negative consequences6 that arise due to uncertainty. In other words, risk can be seen as the potential loss resulting from the interaction with uncertainty, an interaction that occurs according to the degree of exposure7 to chance. Uncertainty cannot be changed, but the degree of exposure to it can Bessis (2011).\nAmong the risks that exist in banking, it is up to us to discuss credit risk. Credit risk is the expected loss resulting from a borrower’s incapability to meet its obligation. So, it refers to the amount lost when a borrower fails to make a payment, which includes a principal, interest and collection costs. Credit risk can be understood as the multiplication of its components:\n\\[\nCredit\\ risk=(default\\ probability)*(exposure\\ at\\ default)*(loss\\ rate\\ given\\ default)\n\\] Where default probability is the likelihood of borrowers’ no being able to comply with their obligations. Exposure is the amount to be lost if the borrower where to default. Loss rate given default is the percentage of amount lost after collection efforts.\nLet’s analyze this with an example: Let’s say the borrower wants to buy a car for $100,000 and the bank will fund 90% of the purchase, so the loan amount is equal to $90,000. Up to this day, the borrower has repaid $10,000, so the outstanding balance is $80,000. If the borrower defaults, the exposure at default would be $80,000. Let’s suppose there is evidence that the probability of default is 30%. If the borrower defaults, the bank can sell the car immediately for $70,000. Then, the remaining loss would be $10,000 and the loss given default would be \\(\\$10,000/\\$90,000 = 11.11%\\). So if the bank where to calculate its expected loss at this very moment, it would be \\(30\\% * 11.11\\% * \\$90,000 = \\$3,000\\).\nFinally, I would like you to take away the idea of why effective supervision is important for the banking sector. Gündüz (2020) : The main purpose of surveillance and supervision is to ensure that banks retain sufficient capital against the risks they bear and to ensure that they operate in an environment where reliable conditions are created. Effective surveillance and supervision in banking plays a critical role in ensuring stability in the financial system of every country. It provides the benefits in free market conditions and in the implementation of effective macroeconomic policies.\nIn the next article in this series of studies we will delve into how these concepts are interrelated, their implications for the banking industry, and why a good credit scoring system is important for financial institutions."
  },
  {
    "objectID": "posts/visa_garch/index.html",
    "href": "posts/visa_garch/index.html",
    "title": "QRM Case Study: Visa",
    "section": "",
    "text": "El siguiente análisis es motivado a raíz del curso de Quantitative Risk Management impartido por Stefan Bolta, organizado por el Instituto de Finanzas de Santo Domingo."
  },
  {
    "objectID": "posts/visa_garch/index.html#visa",
    "href": "posts/visa_garch/index.html#visa",
    "title": "QRM Case Study: Visa",
    "section": "Visa",
    "text": "Visa\nPara este caso estaremos utilizando la serie de precios de Visa desde el 2008 hasta el 2021. Más abajo se muestra la evolución de los retornos diarios de este instrumento.\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(PerformanceAnalytics)\nlibrary(rugarch)\nlibrary(forecast)\nlibrary(FinTS)\n\nx <- getSymbols(Symbols = 'V', \n           verbose = FALSE,\n           warnings = FALSE,\n           from =\"2008-01-03\", to = \"2021-12-31\",\n           auto.assign = FALSE)\nx.ret <- CalculateReturns(Cl(x), method=\"log\") |> \n  na.omit()\n\nx.ret |> \n  ggplot() +\n  geom_line(aes(x = Index, y = V.Close)) +\n  theme_classic()"
  },
  {
    "objectID": "posts/visa_garch/index.html#volatilidad-incondicional-vs-condicional",
    "href": "posts/visa_garch/index.html#volatilidad-incondicional-vs-condicional",
    "title": "QRM Case Study: Visa",
    "section": "Volatilidad Incondicional vs Condicional",
    "text": "Volatilidad Incondicional vs Condicional\nSe observa una alta volatilidad incondicional. Para todo el período de análisis, este indicador asciende a 0.019. Este tipo de analisis considera cada realización de igual manera, por lo que movimientos de precios que ocurrieron al principio de la serie tienen igual ponderación que aquellos movimientos que sucedieron en el pasado reciente.\n\n\nCode\nx.ret |> \n  ggplot() +\n  geom_line(aes(x = Index, y = V.Close)) +\n  geom_hline(yintercept = sd(x.ret$V.Close), col = 'red', linewidth = 1.3) +\n  geom_hline(yintercept = -sd(x.ret$V.Close), col = 'red', linewidth = 1.3) +\n  theme_classic()"
  },
  {
    "objectID": "posts/visa_garch/index.html#ewma",
    "href": "posts/visa_garch/index.html#ewma",
    "title": "QRM Case Study: Visa",
    "section": "EWMA",
    "text": "EWMA\nUna alternativa más robusta es el uso de una medida de volatilidad que descuenta la ponderación en el tiempo.\n\\[\n\\sigma_t^2 =(1-\\lambda)\\varepsilon_{t-1}^2+\\lambda\\sigma_{t-1}^2\n\\]\nEn general, para datos diarios \\(\\lambda = 0.94\\). A continuación se observa un gráfico donde se puede apreciar que la medida de volatilidad es mayor en aquellos periodos donde las variaciones de precios tienden a mostrar variaciones de mayor escala, y que la misma disminuye en periodos de mayor estabilidad de precios.\n\n\nCode\n# Specs\newma.spec <- ugarchspec(mean.model=list(armaOrder=c(1,1),\n                                        include.mean=FALSE),\n                        variance.model=list(model=\"iGARCH\"), \n                        fixed.pars=list(alpha1=1-0.94,\n                                        omega=0))\n# Fit\newma.fit <- ugarchfit(spec = ewma.spec,\n                      data = x.ret)\n\n# Plot\ndataToPlot <- cbind(residuals(ewma.fit), sigma(ewma.fit))\ncolnames(dataToPlot) <- c(\"Retornos\", \"EWMA\")\nplot.zoo(dataToPlot, main=\"Retornos Diarios\", col=\"blue\")"
  },
  {
    "objectID": "posts/visa_garch/index.html#ewma-1",
    "href": "posts/visa_garch/index.html#ewma-1",
    "title": "QRM Case Study: Visa",
    "section": "EWMA",
    "text": "EWMA\n\n\nCode\n# Specs\newma.spec <- ugarchspec(mean.model=list(armaOrder=c(1,1),\n                                        include.mean=FALSE),\n                        variance.model=list(model=\"iGARCH\"), \n                        fixed.pars=list(alpha1=1-0.94,\n                                        omega=0))\n# Fit\newma.fit <- ugarchfit(spec = ewma.spec,\n                      data = x.ret)\n\n# Plot\ndataToPlot <- cbind(residuals(ewma.fit), sigma(ewma.fit))\ncolnames(dataToPlot) <- c(\"Residual\", \"Volatilidad_condicional\")\nplot.zoo(dataToPlot, main=\"Retornos Diarios\", col=\"blue\")"
  },
  {
    "objectID": "posts/visa_garch/index.html#garch",
    "href": "posts/visa_garch/index.html#garch",
    "title": "QRM Case Study: Visa",
    "section": "GARCH",
    "text": "GARCH\nLos modelos GARCH permiten capturar determinados rezagos de la varianza de la serie en cuestión, añadiendo mayor flexibilidad a la hora de modelar el comportamiento de la volatilidad. Propiamente, estos modelos se especifican como:\n\\[\n\\sigma^2 = w +  \\alpha R_{t-1,t}^2 + \\beta \\sigma_{t-1,t}^2\n\\]\nPara la estimación de estos modelos es necesario espeficicar un modelo ARIMA.\n\n\nCode\npar(mfrow=c(2,1))\n  acf(x.ret)\n  pacf(x.ret)\n\n\n\n\n\nLos resultado de las pruebas de autocorrelación y autocorrelacion parcial sugieren el uso de un modelo ARMA(1,1). Más abajo se muestra el resultado de un modelo GARCH(1,1).\n\n\nCode\n# Specs\ngarch.spec <- ugarchspec()\n# Fit\ngarch.fit <- ugarchfit(spec = garch.spec,\n                      data = x.ret)\n\n# Plot\ndataToPlot <- cbind(residuals(garch.fit), sigma(garch.fit))\ncolnames(dataToPlot) <- c(\"Retornos\", \"GARCH\")\nplot.zoo(dataToPlot, main=\"Retornos Diarios\", col=\"blue\")"
  },
  {
    "objectID": "posts/visa_garch/index.html#garch-1",
    "href": "posts/visa_garch/index.html#garch-1",
    "title": "QRM Case Study: Visa",
    "section": "GARCH",
    "text": "GARCH\n\n\nCode\npar(mfrow=c(2,1))\n  acf(x.ret, main=\"ACF\")\n  pacf(x.ret, main=\"PACF\")"
  },
  {
    "objectID": "posts/visa_garch/index.html#garch-2",
    "href": "posts/visa_garch/index.html#garch-2",
    "title": "QRM Case Study: Visa",
    "section": "GARCH",
    "text": "GARCH\n\n\nCode\n# Specs\ngarch.spec <- ugarchspec()\n# Fit\ngarch.fit <- ugarchfit(spec = garch.spec,\n                      data = x.ret)\n\n# Plot\ndataToPlot <- cbind(residuals(garch.fit), sigma(garch.fit))\ncolnames(dataToPlot) <- c(\"Residual\", \"Volatilidad_condicional\")\nplot.zoo(dataToPlot, main=\"Retornos Diarios\", col=\"blue\")"
  },
  {
    "objectID": "posts/visa_garch/index.html#ewma-vs-garch",
    "href": "posts/visa_garch/index.html#ewma-vs-garch",
    "title": "QRM Case Study: Visa",
    "section": "EWMA vs GARCH",
    "text": "EWMA vs GARCH\nAl comparar los resultado de las volatilidades de los modelos EWMA y GARCH(1,1) se puede apreciar que los resultados son similares (para el caso específico de esta serie). Se sugiere explorar otras versiones de la familia de modelos GARCH que logren capturar de mejor manera la volatilidad de esta serie.\n\n\nCode\nautoplot(merge(sigma(ewma.fit), sigma(garch.fit)), facet = NULL) + \n  ylab(\"Volatilidad Condicional\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/visa_garch/index.html#el-mejor-modelo",
    "href": "posts/visa_garch/index.html#el-mejor-modelo",
    "title": "QRM Case Study: Visa",
    "section": "El “mejor” modelo",
    "text": "El “mejor” modelo\nUna forma de buscar un mejor modelo GARCH es estimar diferentes versiones con distintos parámetros y luego escoger el mejor modelo basado en criterios de informacion (como el AIC).\n\n\nCode\n# Importa funciones\nsource('https://raw.githubusercontent.com/msperlin/GARCH-RAC/master/fcts/garch_fcts.R')\n\ntab_out <- do_arch_test(x = x.ret, max_lag = 5)\n\ndist_to_use <- c('norm','std','jsu') # see rugarch::ugarchspecs\nmodels_to_estimate <- c('sGARCH', 'eGARCH', 'gjrGARCH') # see rugarch::rugarchspec help for more\n\nout <- find_best_arch_model(x = x.ret, \n                            type_models = models_to_estimate,\n                            dist_to_use = dist_to_use,\n                            max_lag_AR = 1,\n                            max_lag_MA = 1,\n                            max_lag_ARCH = 1,\n                            max_lag_GARCH = 1)\n\n# tabla resumen con resultados\ntab_out <- out$tab_out\nbest_models <- c(tab_out$model_name[which.min(tab_out$AIC)],\n                 tab_out$model_name[which.min(tab_out$BIC)])\n\ntab_out |> \n  filter(!is.na(AIC)) |> \n  DT::datatable()\n\n\n\n\n\n\n\nCode\n#print(tab_out)\n\n\nLa tabla anterior muestra diferentes resultados de una batería de modelos. De este ejercicio se identifica que los modelos con menor criterio de informacion AIC y BIC son ARMA(1,1)+eGARCH(1,1) jsu, ARMA(1,1)+eGARCH(1,1) jsu. Utilizando esta especificación y los datos observados de las variaciones logarítmicas de precios obtenemos una mejor visión acerca de los riesgos de este instrumento.\n\n\nCode\nbest_spec = ugarchspec(variance.model = list(model =  out$best_bic$type_model, \n                                             garchOrder = c(out$best_bic$lag_arch,\n                                                            out$best_bic$lag_garch)),\n                       mean.model = list(armaOrder = c(out$best_bic$lag_ar, \n                                                       out$best_bic$lag_ma)),\n                       distribution = out$best_bic$type_dist)\n\n# Fit\nmy_best_garch <- ugarchfit(spec = best_spec, \n                           data = x.ret)\n\n# Plot\nplot(my_best_garch, which=\"all\")\n\n\n\nplease wait...calculating quantiles..."
  },
  {
    "objectID": "posts/visa_garch/index.html#simulación-de-monte-carlo",
    "href": "posts/visa_garch/index.html#simulación-de-monte-carlo",
    "title": "QRM Case Study: Visa",
    "section": "Simulación de Monte Carlo",
    "text": "Simulación de Monte Carlo\nCon el “mejor” modelo que describe el comportamiento de la volatilidad de los retornos es posible construir una simulación de Monte Carlo para generar t días en el futuro n cantidad de veces. Utilizando esto, es posible analizar la distribución de estas proyecciones.\n\n\nCode\ndays.ahead = 365\nn.sim <- 1000\ngarch.sim <- matrix(nrow = days.ahead, ncol=n.sim)\n\nset.seed(123)\nfor(i in 1:n.sim){\n  p.sim = ugarchsim(my_best_garch, n.sim=days.ahead, startMethod=\"sample\")\n  garch.sim[,i] <- p.sim@simulation$seriesSim\n}\n\ngarch.sim <- as.data.frame(garch.sim)\n\n# Determinar los percentiles correspondientes\ngarch.sim$Q01 <- NA\ngarch.sim$Q05 <- NA\ngarch.sim$Q25 <- NA\ngarch.sim$Q75 <- NA\ngarch.sim$Q95 <- NA\ngarch.sim$Q99 <- NA\n\ngarch.sim$Q01 <- apply(garch.sim[,2:(ncol(garch.sim)-6)], FUN = function(x){quantile(na.omit(x),0.01)}, MARGIN = 1)\ngarch.sim$Q05 <- apply(garch.sim[,2:(ncol(garch.sim)-6)], FUN = function(x){quantile(na.omit(x),0.05)}, MARGIN = 1)\ngarch.sim$Q25 <- apply(garch.sim[,2:(ncol(garch.sim)-6)], FUN = function(x){quantile(na.omit(x),0.25)}, MARGIN = 1)\ngarch.sim$Q75 <- apply(garch.sim[,2:(ncol(garch.sim)-6)], FUN = function(x){quantile(na.omit(x),0.75)}, MARGIN = 1)\ngarch.sim$Q95 <- apply(garch.sim[,2:(ncol(garch.sim)-6)], FUN = function(x){quantile(na.omit(x),0.95)}, MARGIN = 1)\ngarch.sim$Q99 <- apply(garch.sim[,2:(ncol(garch.sim)-6)], FUN = function(x){quantile(na.omit(x),0.99)}, MARGIN = 1)\n\n# PRICE LEVEL FORECAST ----\n# Mapeo de predicciones de volatilidad por vía de la simulación en el precio\n# sim.dates <- seq(lubridate::ymd(last(index(x.ret))+1), \n#                  last(index(x.ret)) + (days.ahead), \n#                  by = \"days\")\nsim.dates <- seq(last(index(x.ret)) - (days.ahead), \n                 last(index(x.ret)) -1, \n                 by = \"days\")\n\n\n\ndf <- cbind(as.data.frame(sim.dates), garch.sim)\ndf <- as.xts(df[,-1], order.by=df$sim.dates)\ndf <- as.numeric(tail(Cl(x.ret),1)) * cumprod(1 + df)\n\ndf.combined <- rbind(Cl(x.ret))#,Cl(H1.oos))\ndf.combined <- merge(df.combined,df[,1:(ncol(df)-1)]) # do not merge the empty column\n\nlast.day <- index(tail(Cl(x.ret),1))\ndf.combined$Q01 <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-1)], FUN = function(x){quantile(na.omit(x),0.01)}, MARGIN = 1))\ndf.combined$Q05 <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-2)], FUN = function(x){quantile(na.omit(x),0.05)}, MARGIN = 1))\ndf.combined$Q25 <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-3)], FUN = function(x){quantile(na.omit(x),0.25)}, MARGIN = 1))\ndf.combined$Q75 <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-4)], FUN = function(x){quantile(na.omit(x),0.75)}, MARGIN = 1))\ndf.combined$Q95 <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-5)], FUN = function(x){quantile(na.omit(x),0.95)}, MARGIN = 1))\ndf.combined$Q99 <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-6)], FUN = function(x){quantile(na.omit(x),0.99)}, MARGIN = 1))\ndf.combined$mean <- as.numeric(apply(df.combined[,2:(ncol(df.combined)-7)], FUN = function(x){mean(na.omit(x))}, MARGIN = 1))\n\n#tail(df.combined[,c('Q01','Q05','Q25','mean','Q75','Q95','Q99')],15)\n\n#Visualizacion\npar(mfrow=c(1,1))\ngarch.sim.plot <- plot(Cl(df.combined[\"2018::\"]), ylim=c(min(na.omit(df.combined[\"2019::\",1]))*0.6,max(na.omit(df.combined[\"2019::\",1]))*1.3), \n                       main=paste0(\"Simulated \", \n                                   out$best_bic$type_model, \"(\", out$best_bic$lag_arch, \",\" ,out$best_bic$lag_garch, \") + ARMA(\",out$best_bic$lag_ar, \",\" ,out$best_bic$lag_ma,\") + \", out$best_bic$type_dist, \" distribution\"), grid.col=NA)\ngarch.sim.plot <- lines(df.combined[,2:(ncol(df.combined)-6)], col=alpha(\"grey\",0.5), on=1, lty=2, lwd=0.75)\ngarch.sim.plot <- lines(df.combined[,'Q01'], col=\"red\", on=1, lty=1, lwd=1)\ngarch.sim.plot <- lines(df.combined[,'Q05'], col=\"red\", on=1, lty=2, lwd=1.5)\ngarch.sim.plot <- lines(df.combined[,'Q25'], col=\"red\", on=1, lty=3, lwd=1.5)\ngarch.sim.plot <- lines(df.combined[,'mean'], col=\"black\", on=1, lty=2, lwd=1.5)\ngarch.sim.plot <- lines(df.combined[,'Q75'], col=\"blue\", on=1, lty=3, lwd=1.5)\ngarch.sim.plot <- lines(df.combined[,'Q95'], col=\"blue\", on=1, lty=2, lwd=1.5)\ngarch.sim.plot <- lines(df.combined[,'Q99'], col=\"blue\", on=1, lty=1, lwd=1)\ngarch.sim.plot <- lines(Cl(df.combined[\"2021::\"]), col=\"black\", on=1, lwd=1.25)\ngarch.sim.plot <- points(tail(df.combined[,'mean'],1), col=\"red\", pch=16)\ngarch.sim.plot"
  },
  {
    "objectID": "posts/visa_garch/index.html#qrm",
    "href": "posts/visa_garch/index.html#qrm",
    "title": "QRM Case Study: Visa",
    "section": "QRM",
    "text": "QRM\nUno de los objetivos de un Gerente de Riesgo Cuantitativo (QRM) es medir y monitorear el riesgo de un portafolio. Para esta tarea se utiliza la volatilidad de los rendimientos del portafolio como medida de riesgo.\nEn este análisis se exploran diferentes medidas de volatilidad utilizando los retornos de las acciones de la compañía Visa para el período comprendido entre 2008 y 2021.\nMedidas de Volatilidad\n\nNo condicional vs condicional\nExponentially weighted moving average (EWMA)\nGARCH"
  }
]